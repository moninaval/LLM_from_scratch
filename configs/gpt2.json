{
  "model_name": "gpt2",
  "tokenizer_path": "gpt2",
  "tokenizer_type": "huggingface",
  "checkpoint_dir": "checkpoints/gpt2",

  "hidden_size": 768,
  "num_heads": 12,
  "num_layers": 12,
  "intermediate_size": 3072,
  "max_position_embeddings": 1024,
  "vocab_size": 50257,

  "use_rotary": false,
  "norm_type": "layernorm",
  "ffn_type": "gelu",
  "qkv_proj": "split",
  "tie_weights": true,

  "batch_size": 4,
  "lr": 3e-4,
  "epochs": 3,
  "loss": "cross_entropy",
  "device": "cuda",
  "pretokenized": true
}
